{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb047e-b62f-48ee-8ba7-de52945360d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"api_key\")\n",
    "\n",
    "\n",
    "def get_completion(prompt):\n",
    "    \"\"\"\n",
    "    Get the completion for a given prompt using the specified model.\n",
    "    Returns the answer with the highest score.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "            \"models/gemini-2.0-flash\",\n",
    "            system_instruction=\"You are a user.\",\n",
    "        )\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f52131-fde3-4e09-b53e-e31ffc0ca995",
   "metadata": {},
   "source": [
    "# Strategy: “Chain-of-Thought Prompting” (CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225c34c-725f-4231-b209-f7e52a54ecc9",
   "metadata": {},
   "source": [
    "Guiding the model through a step-by-step reasoning process. Instead of prompting the model to directly provide an answer\n",
    "\n",
    "Give the model \"time to think\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bfe8b-ae06-4469-9bb2-5238cda8f849",
   "metadata": {},
   "source": [
    "## Tactic: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27e623-d5a6-46a3-8801-c26f8ffb4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "In a charming village, siblings Jack and Jill set out on \n",
    "a quest to fetch water from a hilltop \n",
    "well. As they climbed, singing joyfully, misfortune \n",
    "struck—Jack tripped on a stone and tumbled \n",
    "down the hill, with Jill following suit. \n",
    "Though slightly battered, the pair returned home to \n",
    "comforting embraces. Despite the mishap, \n",
    "their adventurous spirits remained undimmed, and they \n",
    "continued exploring with delight.\n",
    "\"\"\"\n",
    "\n",
    "# example 1\n",
    "prompt = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following\n",
    "keys: french_summary, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e97aa0-bf40-42db-bdaf-a68490e5a50c",
   "metadata": {},
   "source": [
    "## Tactic: Ask for output in a specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34417f-a558-4c72-8a37-14a2ab4e660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Summarize the following text delimited by \n",
    "  <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the \n",
    "  following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in summary>\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00b9f9-2e10-4a9d-a48f-93cde25ad815",
   "metadata": {},
   "source": [
    "## Tactic: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08714a-c05c-46c5-9e84-33e5f4c977bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "A bakery sells two types of pastries: croissants and muffins. \n",
    "Croissants cost $3 each, and muffins cost $2 each. \n",
    "Yesterday, the bakery sold a total of 50 pastries and made $130 in revenue. \n",
    "How many croissants and muffins did the bakery sell?\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the number of croissants the baker sold, and y be the number of muffins he sold.\n",
    "than x+y=50\n",
    "3x+2y=130\n",
    "So, the bakery sold 20 croissants and 30 muffins\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d464c-8210-4252-a06f-56549117d71c",
   "metadata": {},
   "source": [
    "***Note that the student's solution is actually not correct.***\n",
    "\n",
    "***We can fix this by instructing the model to work out its own solution first.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8530eb-e213-4432-a314-d8e7c4899238",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution \n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem including the final total. \n",
    "- Then compare your solution to the student's solution \n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "A bakery sells two types of pastries: croissants and muffins. \n",
    "Croissants cost $3 each, and muffins cost $2 each. \n",
    "Yesterday, the bakery sold a total of 50 pastries and made $130 in revenue. \n",
    "How many croissants and muffins did the bakery sell?\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "Let x be the number of croissants the baker sold, and y be the number of muffins he sold.\n",
    "than x+y=50\n",
    "3x+2y=130\n",
    "So, the bakery sold 20 croissants and 30 muffins\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c1aca-0718-4022-95e8-b1ff673740a7",
   "metadata": {},
   "source": [
    "# Strategy: Avoid Model Hallucinations\n",
    "- Boie is a real company, the product name is not real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918a319-0cbd-4fd0-b37d-d7280d12ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91019b-29a3-48bc-ad22-c9d7b6a1b79d",
   "metadata": {},
   "source": [
    "## Tactic: Give the model option to say it doesn't know the answer to a question\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e6d59-a600-4e58-8b38-72d2cc810251",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie. Only answer if you know the answer with certainty.\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6226e999f04321",
   "metadata": {},
   "source": [
    "## Tactic: Ask the model to find evidence before answering\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aef52a349b08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie. Find evidence before answering in https://boieusa.com.  \n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe07e9a0f893ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
