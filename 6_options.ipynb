{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecd03a8-1305-4758-a822-fe30c795391b",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb62c2-1055-426c-a884-3c1c30a249f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"api_key\")\n",
    "\n",
    "\n",
    "def get_completion(prompt, temperature=1.0, max_output_tokens=None, stop_sequences=None):\n",
    "    \"\"\"\n",
    "    Get the completion for a given prompt using the specified model.\n",
    "    Returns the answer with the highest score.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "            \"models/gemini-2.0-flash\",\n",
    "            system_instruction=\"You are a user.\",\n",
    "        )\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "                temperature=temperature,\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                stop_sequences=stop_sequences\n",
    "            ),\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115c62959ff62d3",
   "metadata": {},
   "source": [
    "## Temperature: Adding Randomness to the Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775bcce60f780cd",
   "metadata": {},
   "source": [
    "A higher value, makes the answers more diverse, while a lower value, makes them more focused and deterministic.\n",
    "\n",
    "Range for gemini-1.5-flash : 0.0 - 2.0 (default: 1.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3c099-11a1-4955-809b-53e99faab4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Suggest a unique activity for a weekend adventure.\"\"\"\n",
    "\n",
    "print(get_completion(prompt, temperature=0.0)) # vs 2.0 or 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ab16e-7467-4c04-91ff-ad747e99670c",
   "metadata": {},
   "source": [
    "## Max Tokens: Limiting the Response Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea16ded-c2b2-4c52-baca-78036e3b6786",
   "metadata": {},
   "source": [
    "* The max_tokens parameter allows you to limit the length of the generated response\n",
    "* Rule of thumb: 1 token is approximately 4 characters or 0.75 words for English text.\n",
    "* Itâ€™s essential to remember that tokens are not equivalent to words, as they can also represent punctuation marks, spaces, or special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c7ac5-60a9-4125-921f-7250e14177c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Suggest a unique activity for a weekend adventure.\"\n",
    "print(get_completion(prompt, max_output_tokens=100)) # vs 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7b5f1-a0b1-435e-9b55-8cd9ddaeec27",
   "metadata": {},
   "source": [
    "## Stop: Customizing stop Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec39192-f690-4bd1-948a-d07b2a82d221",
   "metadata": {},
   "source": [
    "Providing a list of stop words can help prevent the model from generating responses containing those specific words. Include relevant stop words based on your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc8db1-b532-4d86-b05d-ba528c6f3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "print(get_completion(prompt, stop_sequences=[\"comment\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bff8ad5bb110a",
   "metadata": {},
   "source": [
    "## candidate_count: Number of generated responses to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e829b45aa9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"api_key\")\n",
    "\n",
    "\n",
    "def get_completion(prompt, temperature=1.0, candidate_count=1):\n",
    "    \"\"\"\n",
    "    Get the completion for a given prompt using the specified model.\n",
    "    Returns the answer with the highest score.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "            \"models/gemini-2.0-flash\",\n",
    "            system_instruction=\"You are a user.\",\n",
    "        )\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "                temperature=temperature,\n",
    "                candidate_count=candidate_count\n",
    "            ),\n",
    "    )\n",
    "    return response.candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21aba0503372d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Suggest a unique activity for a weekend adventure.\"\"\"\n",
    "\n",
    "candidates = get_completion(prompt, temperature=1.0, candidate_count=3) #3\n",
    "for candidate in candidates:\n",
    "    print(candidate.content.parts[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a5fed31023b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
