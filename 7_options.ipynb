{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecd03a8-1305-4758-a822-fe30c795391b",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd509deb-e397-4951-8cba-81fc6592a626",
   "metadata": {},
   "source": [
    "## Temperature: Adding Randomness to the Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2200592-65db-446b-87e5-31ee8bb01e2c",
   "metadata": {},
   "source": [
    "A higher value, such as 0.8, makes the answers more diverse, while a lower value, like 0.2, makes them more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8eb62c2-1055-426c-a884-3c1c30a249f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"<your key>\")\n",
    "\n",
    "\n",
    "def get_completion(prompt):\n",
    "    \"\"\"\n",
    "    Get the completion for a given prompt using the specified model.\n",
    "    Returns the answer with the highest score.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "            \"models/gemini-1.5-flash\",\n",
    "            system_instruction=\"You are a user.\",\n",
    "            temperature=1.2\n",
    "        )\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c3c099-11a1-4955-809b-53e99faab4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return []\n",
      "    elif n == 1:\n",
      "        return [0]\n",
      "    elif n == 2:\n",
      "        return [0, 1]\n",
      "    else:\n",
      "        fibseq = [0, 1]\n",
      "        while len(fibseq) < n:\n",
      "            fibseq.append(fibseq[-1] + fibseq[-2])\n",
      "        return fibseq\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Generate a Python function that calculates the fibonacci. \n",
    "    Print only the function without any explanation\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ab16e-7467-4c04-91ff-ad747e99670c",
   "metadata": {},
   "source": [
    "## Max Tokens: Limiting the Response Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea16ded-c2b2-4c52-baca-78036e3b6786",
   "metadata": {},
   "source": [
    "* The max_tokens parameter allows you to limit the length of the generated response\n",
    "* Rule of thumb: 1 token is approximately 4 characters or 0.75 words for English text.\n",
    "* Itâ€™s essential to remember that tokens are not equivalent to words, as they can also represent punctuation marks, spaces, or special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363e9dd0-447c-4e08-8694-0b40f135cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"<your key>\")\n",
    "\n",
    "\n",
    "def get_completion(prompt):\n",
    "    \"\"\"\n",
    "    Get the completion for a given prompt using the specified model.\n",
    "    Returns the answer with the highest score.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "            \"models/gemini-1.5-flash\",\n",
    "            system_instruction=\"You are a user.\",\n",
    "            max_output_tokens=100 # Limit the response length (10)\n",
    "        )\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d07c7ac5-60a9-4125-921f-7250e14177c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"Hello, how are you?\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5aabdf-422f-4d69-9095-d6ed796730ba",
   "metadata": {},
   "source": [
    "## Top P (Nucleus Sampling): Controlling Response Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4475129-8829-4fc8-a37d-f99ac411c470",
   "metadata": {},
   "source": [
    "The top_p parameter controls the diversity and quality of the responses.\n",
    "* A value between 0.3 and 0.9 is recommended. \n",
    "* Higher values (e.g., 0.9) make the model consider a broader range of possibilities\n",
    "* Lower values (e.g., 0.3) make it more selective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9270107b-f7f9-46ea-8eda-0eb1ae458021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"<your key>\")\n",
    "\n",
    "\n",
    "def get_completion(prompt):\n",
    "    \"\"\"\n",
    "    Get the completion for a given prompt using the specified model.\n",
    "    Returns the answer with the highest score.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "            \"models/gemini-1.5-flash\",\n",
    "            system_instruction=\"You are a user.\",\n",
    "            top_p=0.5 # Controls diversity; higher values include more diverse tokens (0.9)\n",
    "        )\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bba94a3-c6c3-4c70-8a36-97772e641412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be delighted to!\n",
      "\n",
      "Here's a potential title for an adventure book:\n",
      "\n",
      "\"The Lost City of Echoes: A Quest Through the Shadows\"\n",
      "\n",
      "This title suggests a thrilling adventure that takes readers on a journey through mysterious lands, hidden cities, and ancient secrets. The use of \"Echoes\" implies a connection to the past, while \"A Quest Through the Shadows\" hints at danger, mystery, and perhaps even supernatural elements.\n",
      "\n",
      "What do you think? Would you like me to come up with more options or modify this one?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a title for an adventure book\"\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7b5f1-a0b1-435e-9b55-8cd9ddaeec27",
   "metadata": {},
   "source": [
    "## Stop: Customizing stop Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec39192-f690-4bd1-948a-d07b2a82d221",
   "metadata": {},
   "source": [
    "Providing a list of stop words can help prevent the model from generating responses containing those specific words. Include relevant stop words based on your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f84988a-63bc-4947-81a1-3f1c59216dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"<your key>\")\n",
    "\n",
    "\n",
    "def get_completion(prompt):\n",
    "    \"\"\"\n",
    "    Get the completion for a given prompt using the specified model.\n",
    "    Returns the answer with the highest score.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "            \"models/gemini-1.5-flash\",\n",
    "            system_instruction=\"You are a user.\",\n",
    "            stop=[\"Bonjour\", \"###\"]  # Stops generating when these words are encountered\n",
    "        )\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aacc8db1-b532-4d86-b05d-ba528c6f3f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"Hello, how are you?\" in French is:\n",
      "\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0056c202-ecb3-41b2-998a-505aabfd3f59",
   "metadata": {},
   "source": [
    "## Frequency Penalty: Controlling Repetitive Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb52bcd-a5c8-421e-9240-97057397136f",
   "metadata": {},
   "source": [
    "* Control the model's tendency to generate repetitive responses. \n",
    "* Higher values, like 1.0, encourage the model to explore more diverse and novel responses\n",
    "* Lower values, such as 0.2, make the model more likely to repeat information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "379ef7c2-cc4a-43ae-be89-127b4dcf971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"<your key>\")\n",
    "\n",
    "\n",
    "def get_completion(prompt):\n",
    "    \"\"\"\n",
    "    Get the completion for a given prompt using the specified model.\n",
    "    Returns the answer with the highest score.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "            \"models/gemini-1.5-flash\",\n",
    "            system_instruction=\"You are a user.\",\n",
    "            frequency_penalty=0  # Discourages repetition of frequently used words (2)\n",
    "        )\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc5cb620-f223-4971-851f-842a814f38ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Frequencies (excluding conjunctions) with count > 2:\n",
      "exercise: 11\n",
      "benefits: 7\n",
      "can: 6\n",
      "regular: 6\n",
      "overall: 5\n",
      "physical: 5\n",
      "1: 5\n",
      "a: 5\n",
      "2: 5\n",
      "improved: 5\n",
      "health: 4\n",
      "reduce: 4\n",
      "activity: 4\n",
      "improving: 3\n",
      "mental: 3\n",
      "levels: 3\n",
      "weight: 3\n",
      "risk: 3\n",
      "3: 3\n",
      "has: 3\n",
      "been: 3\n",
      "confidence: 3\n",
      "s: 3\n",
      "social: 3\n",
      "increased: 3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "prompt = \"Describe the benefits of exercise.\"\n",
    "\n",
    "# List of common conjunctions\n",
    "conjunctions = {\"to\", \"by\", \"the\", \"of\", \"and\", \"or\", \"but\", \"nor\", \"for\", \"yet\", \"so\", \"after\", \"although\", \"as\", \"because\", \"before\", \"even\", \"if\", \"once\", \"since\", \"though\", \"unless\", \"until\", \"when\", \"where\", \"while\"}\n",
    "\n",
    "def remove_conjunctions(text):\n",
    "    # Split the text into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Filter out conjunctions\n",
    "    filtered_words = [word for word in words if word not in conjunctions]\n",
    "    return filtered_words\n",
    "\n",
    "def count_word_frequencies(words):\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts\n",
    "\n",
    "def main():\n",
    "    # Get input from the user\n",
    "    text = get_completion(prompt)\n",
    "    \n",
    "    # Remove conjunctions and get the list of words\n",
    "    words = remove_conjunctions(text)\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = count_word_frequencies(words)\n",
    "    \n",
    "    # Sort the word counts by frequency in descending order\n",
    "    sorted_word_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Print the results for words with frequency greater than 2\n",
    "    print(\"\\nWord Frequencies (excluding conjunctions) with count > 2:\")\n",
    "    for word, count in sorted_word_counts:\n",
    "        if count > 2:\n",
    "            print(f\"{word}: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c463f-9596-4da2-a2ae-27f2215a954c",
   "metadata": {},
   "source": [
    "## Presence Penalty: Controlling Avoidance of Certain Topics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712cc3d8-7d80-4113-a044-671e2699b9b9",
   "metadata": {},
   "source": [
    "* Allows you to influence the model's avoidance of specific topics in its responses. \n",
    "* Higher values, such as 1.0, make the model more likely to avoid mentioning particular topics provided in the user message\n",
    "* Lower values, like 0.2, make the model less concerned about preventing those topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f13ddc4a-47bd-41a6-8abc-684b21e0fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"<your key>\")\n",
    "\n",
    "\n",
    "def get_completion(prompt):\n",
    "    \"\"\"\n",
    "    Get the completion for a given prompt using the specified model.\n",
    "    Returns the answer with the highest score.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "            \"models/gemini-1.5-flash\",\n",
    "            system_instruction=\"You are a user.\",\n",
    "            presence_penalty=1  # Encourages new words and ideas (0.2)\n",
    "        )\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ded80279-4dfb-45b2-a0fa-eb1cc26914bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ideas, assumptions and biases**\n",
      "\n",
      "PromptHub is a platform that helps you validate your ideas by testing them with real people. It's designed to make sure you're building something that resonates with your target audience.\n",
      "\n",
      "Is that what you were looking for?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"PromptHub makes it easy test __ \"\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f21328-2a4a-4223-ba2a-f44e52e6a73c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
